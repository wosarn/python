{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://sigdelta.com/assets/images/sages-sd-logo.png)\n",
    "\n",
    "# Analiza danych tekstowych w Python\n",
    "\n",
    "\n",
    "## Gensim i word embeddings\n",
    "\n",
    "W tym notebooku przedstawiamy pakiet Gensim w kontekście word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trenowanie własnych w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykorzystamy ten sam korpus co przy nauce tagerów w poprzednim notatniku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "corpus = nltk.corpus.brown\n",
    "sentences = corpus.sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdania bez dużych liter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_lower = [[w.lower() for w in s] for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2V można trenować korzystając z modelu [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_brown = Word2Vec(sentences, size=100, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_brown_lower = Word2Vec(sentences_lower, size=100, min_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pobranie wektora cech danego słowa jest bardzo proste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_brown[\"system\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Wytrenuj modele w2v wielokości 50 i 200. Wytrenowane modele zachowaj w osobnych zmiennych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytywanie gotowych modeli\n",
    "\n",
    "Gotowe modele można wczytać z pliku korzystając z klasy Word2Vec lub [KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html)\n",
    "\n",
    "Wczytamy gotowy, stosunkowo niewielki model [GloVe](https://nlp.stanford.edu/projects/glove/). Samodzielnie pobrany model należy wstępnie przekonwertować narzędziem [gensim.scripts.glove2word2vec](https://radimrehurek.com/gensim/scripts/glove2word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 100d)\n",
    "w2v_path = \"w2v/glove.6B.100d.w2vformat.txt\"\n",
    "\n",
    "w2v_glove = KeyedVectors.load_word2vec_format(w2v_path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksploracja modelu w2v\n",
    "\n",
    "Najbardziej podobne słowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_glove.most_similar(positive=['girl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_brown.most_similar(positive=['girl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_brown_lower.most_similar(positive=['girl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miara podobieństwa dwóch słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_glove.similarity('dog', 'cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niepasujące słowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_glove.doesnt_match(\"dog cat fish home bird\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Własny tagger korzystający z W2V\n",
    "\n",
    "Podobnie jak w poprzednim notatniku, dzielimy korpus na zbiór treninowy i testowy. Definiujemy pomocnicze funkcje do transformacji danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown_tagged_sents = corpus.tagged_sents(categories='news')\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja generująca cechy zwraca tylko jędną cechę: dane słowo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index].lower(),\n",
    "#         'prev_word': '' if index == 0 else sentence[index - 1].lower(),\n",
    "#         'next_word': '' if index == len(sentence) - 1 else sentence[index + 1].lower()\n",
    "    }\n",
    "\n",
    "X, y = transform_to_dataset(train_sents)\n",
    "X_test, y_test = transform_to_dataset(test_sents)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stworzymy niestandardowy `vectorizer` rozszerzający [W2VTransformer](https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html). Umożliwia on konkatenację wektorów w2v poszczególnych cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "from gensim.sklearn_api.w2vmodel import W2VTransformer\n",
    "\n",
    "class CustomTransformer(W2VTransformer):\n",
    "    def __init__(self, gensim_model):\n",
    "        self.gensim_model = gensim_model\n",
    "        self.size = gensim_model.vector_size\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, words):\n",
    "        \n",
    "        X = [[] for _ in range(0, len(words))]\n",
    "        feat_num = len(words[0])\n",
    "        for k, v in enumerate(words):\n",
    "            vectors = []\n",
    "            for f_key, f_val in v.items():\n",
    "                vectors.append(self._transform_word(f_val)) \n",
    "            X[k] = np.concatenate(vectors)\n",
    "\n",
    "        return np.reshape(np.array(X), (len(words), self.size*feat_num))\n",
    "\n",
    "    def _transform_word(self, word):\n",
    "        try:\n",
    "            return self.gensim_model[word]\n",
    "        except KeyError:\n",
    "            return np.zeros(self.size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pora na naukę klasyfikatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    " \n",
    "clf = Pipeline([\n",
    "    ('vectorizer', CustomTransformer(w2v_glove)),\n",
    "#     ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "    ('classifier', svm.LinearSVC())\n",
    "])\n",
    "\n",
    "limit = 10000 # Use only the first 10K samples \n",
    " \n",
    "clf.fit(X[:limit], y[:limit])   \n",
    "print(\"Classifier ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Porównaj skuteczność klasyfikacji z wykorzystaniem samodzielnie nauczonego i gotowego modelu w2v.\n",
    "2. Sprawdź wpływ wielkości wektora samodzielnie nauczonych modeli w2v.\n",
    "3. Sprawdź czy wykorzystanie poprzedniego i następnego słowa do generowania cech poprawia skuteczność klasyfikacji. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
