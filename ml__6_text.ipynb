{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://sigdelta.com/assets/images/sages-sd-logo.png)\n",
    "\n",
    "# Analiza danych i uczenie maszynowe w Python\n",
    "\n",
    "Autor notebooka: Jakub Nowacki.\n",
    "\n",
    "## Dane tekstowe\n",
    "\n",
    "W tym notebooku prezentujemy podejście do analizy, wektoryzacji danych i uczenia maszynowego z użyciem scikit-learn i innych pakietów w Python. Kroki te są wstępem do uczenia maszynowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Parametry wykresów\n",
    "mpl.style.use('ggplot')\n",
    "mpl.rcParams['figure.figsize'] = (8,6)\n",
    "mpl.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macierz Term-Document\n",
    "\n",
    "[Macierz Term-Document (TD)](https://en.wikipedia.org/wiki/Document-term_matrix), jest to macierz, w której kolumny stanowią unikalne słowa, a wiersze stanowią wartości ile razy dane słowo wystąpiło w dokumencie. Dla przykładu (za Wikipedią), jeżeli mamy dwa dokumenty:\n",
    "\n",
    "* D1 = \"I like databases\"\n",
    "* D2 = \"I hate databases\"\n",
    "\n",
    "to macierz TD możemy przedstawić jako\n",
    "\n",
    "|    | I | like | hate | databases |\n",
    "|----|---|------|------|-----------|\n",
    "| **D1** | 1 | 1    | 0    | 1         |\n",
    "| **D2** | 1 | 0    | 1    | 1         |\n",
    "\n",
    "Poniżej ten sam przykład zrealizowany w Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>I like databases Databases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>I hate databases</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docs                       lines\n",
       "0   D1  I like databases Databases\n",
       "1   D2            I hate databases"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.DataFrame({\n",
    "    'docs': ['D1', 'D2'],\n",
    "    'lines': ['I like databases Databases', 'I hate databases']\n",
    "})\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>lines</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>I like databases Databases</td>\n",
       "      <td>[i, like, databases, databases]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>I hate databases</td>\n",
       "      <td>[i, hate, databases]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docs                       lines                            words\n",
       "0   D1  I like databases Databases  [i, like, databases, databases]\n",
       "1   D2            I hate databases             [i, hate, databases]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['words'] = sample.lines.str.strip().str.lower().str.split('[\\W_]+')\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1</td>\n",
       "      <td>databases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1</td>\n",
       "      <td>databases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D2</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D2</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D2</td>\n",
       "      <td>databases</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docs       word\n",
       "0   D1          i\n",
       "1   D1       like\n",
       "2   D1  databases\n",
       "3   D1  databases\n",
       "4   D2          i\n",
       "5   D2       hate\n",
       "6   D2  databases"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = list()\n",
    "for row in sample[['docs', 'words']].iterrows():\n",
    "    r = row[1]\n",
    "    for word in r.words:\n",
    "        rows.append((r.docs, word))\n",
    "\n",
    "words = pd.DataFrame(rows, columns=['docs', 'word'])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>word</th>\n",
       "      <th>databases</th>\n",
       "      <th>hate</th>\n",
       "      <th>i</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "word  databases  hate    i  like\n",
       "docs                            \n",
       "D1          2.0   0.0  1.0   1.0\n",
       "D2          1.0   1.0  1.0   0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.pivot_table(index='docs', \n",
    "                  columns='word', \n",
    "                  aggfunc=lambda v: v['word'].count())\\\n",
    "    .fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Używając Pandas obliczyć macierz TD dla książek.\n",
    "1. Co może wpłynąć na wynik?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I like databases Databases\n",
       "1              I hate databases\n",
       "Name: lines, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x3 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = count_vectorizer.fit_transform(sample['lines'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t2\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 1],\n",
       "       [1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['databases', 'hate', 'like']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>databases</th>\n",
       "      <th>hate</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      databases  hate  like\n",
       "docs                       \n",
       "D1            2     0     1\n",
       "D2            1     1     0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "             columns=count_vectorizer.get_feature_names(), \n",
    "             index=sample['docs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Kolejną techniką jest poznane uprzednio [Term Frequency–Inverse Document Frequency (TF-IDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Jest on popularnym algorytmem do analizy danych tekstowych, używany dość często w pozyskiwaniu danych (data mining).\n",
    "\n",
    "Poniżej przykładowe obliczenie TF-IDF z użyciem wektoryzera scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tfidf_vectorizer.fit_transform(sample['lines'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81818021, 0.        , 0.57496187],\n",
       "       [0.57973867, 0.81480247, 0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['databases', 'hate', 'like']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>databases</th>\n",
       "      <th>hate</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>0.579739</td>\n",
       "      <td>0.814802</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      databases      hate      like\n",
       "docs                               \n",
       "D1     0.818180  0.000000  0.574962\n",
       "D2     0.579739  0.814802  0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "            columns=tfidf_vectorizer.get_feature_names(), \n",
    "            index=sample['docs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miary podobieństwa\n",
    "\n",
    "Jest wiele miar podobieństwa, które liczą jak odległe są pary wartości od siebie. Miary mogą być używane zarówno do wektorów oraz macierzy, jak i słów oraz dokumentów; zobacz [ciekawy opis](http://dataconomy.com/2015/04/implementing-the-five-most-popular-similarity-measures-in-python/) miar wraz z implementacją w Pythonie. \n",
    "\n",
    "Pierwszą miarą, która jest standardową miarą podobieństwa dwóch stringów jest [miara Levenshteina](https://en.wikipedia.org/wiki/Levenshtein_distance). Jest wiele implementacji używania tej miary, niemniej, najłatwiej dostępną implementacją jest zawarta w [difflib](https://docs.python.org/3/library/difflib.html), która jest częścią standardowej biblioteki Pythona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import difflib \n",
    "\n",
    "def similarity(a, b):\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "similarity('cat', 'cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity('cat', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity(sample.at[0, 'lines'], sample.at[1, 'lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity('cat', 'catepillar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku dokumentów tekstowych, znane są przynajmniej dwie popularne miary:\n",
    "\n",
    "* cosinusowa\n",
    "* Jaccarda\n",
    "\n",
    "**Miaria cosinusowa** zdefiniowana jest jako kąt między dwoma wektorami:\n",
    "\n",
    "$$\n",
    "sim(A, B) = \\cos(\\Theta) = \\frac{A \\cdot B}{\\Vert A \\Vert \\cdot \\Vert B \\Vert}\n",
    "$$\n",
    "\n",
    "Zatem miara wymaga formy zwektowyzowanej do obliczenia wartości; musimy się najpierw posłużyć którymś wektoryzatorem aby otrzymać macierz a potem policzyć miarę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = count_vectorizer.fit_transform(sample['lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Miara Jaccarda** operuje z kolei na zbiorach i zdefiniowana jest jako:\n",
    "\n",
    "$$\n",
    "sim(A, B) = \\frac{|A \\cap B|}{| A \\cup B |}\n",
    "$$\n",
    "\n",
    "Zatem dla naszego przykładu wygląda to następująco:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = sample.words[0]\n",
    "B = sample.words[1]\n",
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim_jaccard(A, B):\n",
    "    a = set(A)\n",
    "    b = set(B)\n",
    "    i = set.intersection(a, b)\n",
    "    u = set.union(a, b)\n",
    "    print(i, u)\n",
    "    return len(i)/len(u)\n",
    "\n",
    "sim_jaccard(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "list(set(A))\n",
    "jaccard_similarity_score(list(set(A)), list(set(B)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Transformatory i pipeliny\n",
    "\n",
    "Scikit-learn wprowadził szereg ułatwień do przetwarzania danych i tworzenia modeli. W ogólności, możemy korzystać z 3 podstawowych elementów:\n",
    "\n",
    "* [transformer](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html)\n",
    "* [estimator](http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)\n",
    "* [pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)\n",
    "\n",
    "Transformetry (funkcje zmieniające dane) i estymatory (modele do wyuczenia) łączy się w pipeliny; więcej o tym można przeczytać [w dokumentacji](http://scikit-learn.org/stable/modules/pipeline.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td = CountVectorizer().fit_transform(sample['lines'])\n",
    "tfidf_transformer.fit_transform(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "tfidf_pipeline = make_pipeline(CountVectorizer(), TfidfTransformer())\n",
    "tfidf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tfidf_pipeline.fit_transform(sample['lines'])\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "             index=sample['docs'], \n",
    "             columns=tfidf_pipeline.steps[0][1].get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można też samemu nazywać elementy pipelinu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(steps.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps = {\n",
    "    'count_vect': CountVectorizer(),\n",
    "    'tfidf_trans': TfidfTransformer()\n",
    "}\n",
    "\n",
    "# Pipeline oczekuje kroków jako listy z krotkami (nazwa, obiekt)\n",
    "tfidf_pipeline = Pipeline(list(steps.items()))\n",
    "tfidf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_pipeline.fit_transform(sample['lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Co tu się dzieje?\n",
    "steps['count_vect'].get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Wykorzystaj narzędzia scikit-learn do stworzenie pełnego pipelinu wykonującego wektoryzację z TF-IDF dla danych z książek.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje użytkownika\n",
    "\n",
    "Niekiedy mamy potrzebę korzystania z funkcji wybiegających poza zestaw dostępny w scikit-learn. Są dwie metody:\n",
    "\n",
    "* [FunctionTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer)\n",
    "* Klasa dziedzicząca po [BaseEstimator i TransformerMixin](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py)\n",
    "\n",
    "Obecnie najpierw zalecany jest FunctionTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "@np.vectorize\n",
    "def replace_database(linia):\n",
    "    return re.sub('database', 'DB', linia, flags=re.IGNORECASE)\n",
    "\n",
    "replace_database(sample['lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "replace_func = FunctionTransformer(replace_database, validate=False)\n",
    "replace_func.fit_transform(sample['lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_pipeline = make_pipeline(replace_func, TfidfVectorizer())\n",
    "new_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = new_pipeline.fit_transform(sample['lines'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "             index=sample['docs'], \n",
    "             columns=new_pipeline.steps[1][1].get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Drugą metodą tworzenia funkcji użytkownika jest stworzenie klasy, która dziedziczy po klasach `BaseEstimator` i `TransformerMixin`, czyli de facto jak to jest robione dla innych transformerów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ReplaceDatabaseTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, replace_from='database', replace_to='DB'):\n",
    "        self.replace_from = replace_from\n",
    "        self.replace_to = replace_to\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def _replace_str(self, line):\n",
    "        return re.sub(self.replace_from, \n",
    "                      self.replace_to, \n",
    "                      line, \n",
    "                      flags=re.IGNORECASE)\n",
    "    \n",
    "    def transform(self, data):\n",
    "        func = np.vectorize(lambda line: self._replace_str(line))\n",
    "        return func(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_pipeline2 = make_pipeline(ReplaceDatabaseTransformer(), TfidfVectorizer())\n",
    "new_pipeline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = new_pipeline2.fit_transform(sample['lines'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "             index=sample['docs'], \n",
    "             columns=new_pipeline2.steps[1][1].get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Napisz funkcję zamieniającą wszystkie napotkane [URLe](https://pl.wikipedia.org/wiki/Uniform_Resource_Locator) na stałą wartość, która może być podawana jako argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja danych tekstowych\n",
    "\n",
    "W tej części notebooka przeprowadzimy klasyfikację danych tekstowych. Do analiz będziemy wykorzystywać kolekcję wiadomości SMS, zawierających spam i prawdziwe wiadomości, dostępnym w [repozytorium UCI](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). Poniższy kod pobiera dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "data_path = 'data'\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "file_name = url.split('/')[-1]\n",
    "dest_file = os.path.join(data_path, file_name) \n",
    "\n",
    "data_file = 'SMSSpamCollection'\n",
    "data_full = os.path.join(data_path, data_file)\n",
    "\n",
    "urllib.request.urlretrieve(url, dest_file)\n",
    "\n",
    "with zipfile.ZipFile(dest_file) as zip_file:\n",
    "    zip_file.extract(data_file, path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sms = pd.read_csv(data_full, \n",
    "                  sep='\\t', \n",
    "                  names=['is_spam', 'text'])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sms, test_sms = train_test_split(sms, test_size=0.2)\n",
    "\n",
    "print('Train set:')\n",
    "print(train_sms.describe())\n",
    "print()\n",
    "print('Test set:')\n",
    "print(test_sms.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naszym zadaniem jest wytrenowanie klasyfikatora, który klasyfikuje wiadomość SMS jako spam. Podobne zadanie z dość szerokim opisem można znaleźć w [tym wpisie](https://radimrehurek.com/data_science_python/). \n",
    "\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "Zaczniemy od klasyfikatora [Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html). Używa on [twierdzenia Bayesa](https://en.wikipedia.org/wiki/Bayes%27_theorem) do obliczenia prawdopodobieństw poszczególnych klas w zależności od dostępnych opcji, przy założeniu niezależności zmiennych losowych (stąd naiwny). Rozpatrzmy przykład ([źródło](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)):\n",
    "\n",
    "![](https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_41-850x310.png)\n",
    "\n",
    "Poniżej przykład obliczenia prawdopodobieństwa na podstawie metody Bayesa.\n",
    "\n",
    "$$P(Yes | Sunny) = \\frac{P( Sunny | Yes)  P(Yes)}{P (Sunny)}$$\n",
    "$$P (Sunny |Yes) = 3/9 = 0.33$$\n",
    "$$P(Sunny) = 5/14 = 0.36$$ \n",
    "$$P( Yes)= 9/14 = 0.64$$\n",
    "$$P (Yes | Sunny) = \\frac{0.33 \\cdot 0.64}{0.36} = 0.60$$\n",
    "\n",
    "Poniżej, przykład modelu detekcji spamu z użuciem metody Naive Bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(train_sms['text'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "spam_detector = MultinomialNB().fit(X, train_sms['is_spam'])\n",
    "spam_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 17\n",
    "train_sms.iloc[i, 0], spam_detector.predict(X[i])[0], train_sms.iloc[i, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X = vectorizer.transform(test_sms['text'])\n",
    "\n",
    "y_pred = spam_detector.predict(X)\n",
    "y_true = test_sms['is_spam']\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Zbuduj pipeline wektoryzatora i modelu.\n",
    "1. Użyj `GridSearchCV` do znalezienia najlepszego modelu.\n",
    "1. Popraw tokenizację w celu poprawienia jakości klasyfikacji.\n",
    "1. Użyj innych klasyfikatorów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelowanie tematów\n",
    "\n",
    "Modelowanie tematów (topic modelling) jest to zadanie ekstrakcji ważnych elementów z tekstu, które są jego tematami. Wiodącą biblioteką w pythonie do tego celu jest [gensim](https://radimrehurek.com/gensim/). \n",
    "\n",
    "### Latent Dirichlet Allocation\n",
    "\n",
    "Jest wiele ciekawych algorytmów wykonujących to zadanie, jednak jednym z najgłośniejszych algorytmów do ekstrakcji tematów jest [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). \n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/0*II7wZlKViCt4ssBm.png)\n",
    "\n",
    "Jedną z najskuteczniejszych implementaci LDA w Pythonie jest implementacja dostępna w gensim, która posiada też dość rozbudowaną [dokumentację](https://radimrehurek.com/gensim/models/ldamodel.html). Wykorzystajmy zatem LDA do naszego zadania.\n",
    "\n",
    "Do analizy wykorzystamy zbiór [20 newsgroups z scikit-learn](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                                remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zawęzimy trochę próbkę, żeby uczenie było szybsze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "newsgroups_samples = newsgroups.data[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najpierw tworzymy korpus dla LDA używając narzędzi gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "words = [nltk.regexp_tokenize(d.lower(), '\\w+') for d in newsgroups_samples]\n",
    "\n",
    "dictionary = corpora.Dictionary(words)\n",
    "corpus = [dictionary.doc2bow(text) for text in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Słownik posiada ID słowa jako klucz i słowo jako wartość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for k, v in islice(dictionary.items(), 10):\n",
    "    print('{}: {}'.format(k, v)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korpus posiada listę krotek, lista na dokument; krotki zawierają pary ID słowa i jego liczność w danych dokumencie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Dla 10 pierwszych dokumentów wypisz słowa i ich liczność."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz wytrenujmy sam model LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, alpha=\"auto\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(model.get_document_topics(corpus[i], minimum_probability=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "model.get_topic_terms(topicid=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Zmień ilość tematów; czy coś się zmieniło?\n",
    "1. Zobacz jak tematy mapują się na klasy.\n",
    "1. Popraw tokenizację; możesz np.:\n",
    "    - usunąć nie-słowa\n",
    "    - usunąć wyrazy ze stop listy\n",
    "    - sprowadzić słowa do wspólnej wielkości znaku\n",
    "1. Sprawdź inne metody budowania korpusu, np.:\n",
    "    - macierz TD\n",
    "    - TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.sklearn_api.ldamodel import LdaTransformer\n",
    "from gensim.sklearn_api.text2bow import Text2BowTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = {\n",
    "    'text2bow': Text2BowTransformer(),\n",
    "    'lda': LdaTransformer(num_topics=10)\n",
    "}\n",
    "\n",
    "p = Pipeline(list(steps.items()))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.fit_transform(newsgroups_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps['lda'].gensim_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps['text2bow'].gensim_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.transform(['ala ma kota', 'lala'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Możemy też zastosować wbudowaną metodę scikit-learn; zobacz [dokumentację](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funkcja użytkowa do wyświetlania tematów.\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=1000,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(newsgroups_samples)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Zrób pipeline z powyższego przetwarzania.\n",
    "1. Spróbuj użyć TF-IDF.\n",
    "1. Zmień liczbę tematów `n_components` i zobacz co się zmieni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization\n",
    "\n",
    "Kolejnym algorytmem często używanym do ekstrakcji tematów jest [Non-negative Matrix Factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization). \n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/f/f9/NMF.png)\n",
    "\n",
    "Metoda ta używa faktoryzacji do przybliżenia macierzy V jako iloczynu macierzy H i W. W wyniku działania algorytmu macierze H i W mają właściwości klasteryzacyjne danych w macierzy V. Dokładnie W staje się macierzą centroidów a H indykatorem przyporządkowania do klastrów poszczególnych elementów macierzy V.\n",
    "\n",
    "W praktyce często stosuje się tą metodę jako zamiennik PCA, lecz tylko dla danych pozytywnych, oraz do ekstrakcji tematów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=1000,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(newsgroups_samples)\n",
    "\n",
    "nmf = NMF(n_components=10, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Zmień ilość tematów `n_components`.\n",
    "1. Spróbuj innej funkcji straty `beta_loss`; zobacz [dokumentację](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
